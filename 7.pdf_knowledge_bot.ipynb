{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a714099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and ﬁgures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani→\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer→\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parm\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez→†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "!ukasz Kaiser→\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin→‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural net\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these mo\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU scor\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "→Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and sta\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyMuPDFLoader(\"sample1.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "chunks = splitter.split_documents(pages)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content[:300]}\\n{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6f44e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these mo\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU scor\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "→Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and sta\n",
      "----------------------------------------\n",
      "Chunk 4:\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model varia\n",
      "----------------------------------------\n",
      "Chunk 5:\n",
      "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def clean_chunks(chunks):\n",
    "    filtered = []\n",
    "    for doc in chunks:\n",
    "        text = doc.page_content.strip()\n",
    "\n",
    "        if len(text) < 100:\n",
    "            continue\n",
    "        if any(phrase in text.lower() for phrase in [\"permission\", \"@\", \"license\", \"figure\", \"©\", \"reproduce\"]):\n",
    "            continue\n",
    "        filtered.append(doc)\n",
    "    return filtered\n",
    "\n",
    "cleaned_chunks = clean_chunks(chunks)\n",
    "\n",
    "for i, chunk in enumerate(cleaned_chunks[:5]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content[:300]}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5d06f",
   "metadata": {},
   "source": [
    "now the text is split, so let us embed it and store in faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2dc46a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_model = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(cleaned_chunks, embedding_model)\n",
    "db.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb9aeea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "--------------------------------------------------\n",
      "the approach we take in our model.\n",
      "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5\n",
      "Training\n",
      "This section describes the training regime for our models.\n",
      "5.1\n",
      "Training Data and Batching\n",
      "--------------------------------------------------\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "--------------------------------------------------\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [9] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4\n",
      "Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "db = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"How does the self-attention mechanism work?\"\n",
    "docs = db.similarity_search(query, k=4)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:500])\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a732a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[38, 2, 9].\n",
      "--------------------------------------------------\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3\n",
      "Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "--------------------------------------------------\n",
      "3.3\n",
      "Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0, xW1 + b1)W2 + b2\n",
      "(2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "--------------------------------------------------\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [12]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the transformer encoder work?\"\n",
    "docs = db.similarity_search(query, k=4)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:500])\n",
    "    print(\"-\" * 50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
