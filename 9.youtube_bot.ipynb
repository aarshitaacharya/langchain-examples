{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5817adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fc75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube-transcript-api\n",
      "  Downloading youtube_transcript_api-1.1.1-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.9/485.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting defusedxml<0.8.0,>=0.7.1\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: requests in /Users/aarshitaacharya/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from youtube-transcript-api) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aarshitaacharya/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aarshitaacharya/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aarshitaacharya/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->youtube-transcript-api) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aarshitaacharya/.pyenv/versions/3.10.16/lib/python3.10/site-packages (from requests->youtube-transcript-api) (3.10)\n",
      "Installing collected packages: defusedxml, youtube-transcript-api\n",
      "Successfully installed defusedxml-0.7.1 youtube-transcript-api-1.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b62932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460157c",
   "metadata": {},
   "source": [
    "Step 1 Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bda7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want to tell you about a framework to help large language models be more accurate and more up to date: Retrieval-Augmented Generation, or RAG. Let's just talk about the \"Generation\" part for a minute. So forget the \"Retrieval-Augmented\". So the generation, this refers to large language models, or LLMs, that generate text in response to a user query, referred to as a prompt. These models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question: \"In our solar system, what planet has the most moons?\" And my response was, “Oh, that's really great that you're asking this question. I loved space when I was your age.” Of course, that was like 30 years ago. But I know this! I read an article and the article said that it was Jupiter and 88 moons. So that's the answer. Now, actually, there's a couple of things wrong with my answer. First of all, I have no source to support what I'm saying. So even though I confidently said “I read an article, I know the answer!”, I'm not sourcing it. I'm giving the answer off the top of my head. And also, I actually haven't kept up with this for awhile, and my answer is out of date. So we have two problems here. One is no source. And the second problem is that I am out of date.   And these, in fact, are two behaviors that are often observed as problematic when interacting with large language models. They’re LLM challenges. Now, what would have happened if I'd taken a beat and first gone and looked up the answer on a reputable source like NASA? Well, then I would have been able to say, “Ah, okay! So the answer is Saturn with 146 moons.” And in fact, this keeps changing because scientists keep on discovering more and more moons. So I have now grounded my answer in something more \n",
      "believable. I have not hallucinated or made up an answer. Oh, by the way, I didn't leak personal information about how long ago it's been since I was obsessed with space. All right, so what does this have to do with large language models? Well, how would a large language model have answered this question? So let's say that I have a user asking this question about moons. A large language model would confidently say, OK, I have been trained and from what I know in my parameters during my training, the answer is Jupiter. The answer is wrong. But, you know, we don't know. The large language model is very confident in what it answered. Now, what happens when you add this retrieval augmented part here? What does that mean? That means that now, instead of just relying on what the LLM knows, we are adding a content store. This could be open like the internet. This can be closed like some collection of documents, collection of policies, whatever. The point, though, now is that the LLM first goes and talks to the content store and says, “Hey, can you retrieve for me information that is relevant to what the user's query was?” And now, with this retrieval-augmented answer, it's not Jupiter anymore. We know that it is Saturn. What does this look like? Well, first user prompts the LLM with their question. They say, this is what my question was. And originally, if we're just talking to a generative model, the generative model says, “Oh, okay, I know the response. Here it is. Here's my response.”   But now in the RAG framework, the generative model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve relevant content.\" \"Combine that with the user's question and only then generate the answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved content, together with the user's question. Now give a response. And in fact, now you can give evidence for why your response was what it was.   So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?   So first of all, I'll start with the out of date part. Now, instead of having to retrain your model, if new information comes up, like, hey, we found some more moons-- now to Jupiter again, maybe it'll be Saturn again in the future. All you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we're ready. We just go ahead and retrieve the most up to date information. The second problem, source. Well, the large language model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, “I don't know.” If the user's question cannot be reliably answered based on your data store, the model should say, \"I don't know,\" instead of making up something that is believable and may mislead the user. This can have a negative effect as well though, because if the retriever is not sufficiently good to give the large language model the best, most high-quality grounding information, then maybe the user's query that is answerable doesn't get an answer. So this is actually why lots of folks, including many of us here at IBM, are working the problem on both sides. We are both working to improve the retriever to give the large language model the best quality data on which to ground its response, and also the generative part so that the LLM can give the richest, best response finally to the user when it generates the answer. Thank you for learning more about RAG and like and subscribe to the channel. Thank you.\n"
     ]
    }
   ],
   "source": [
    "video_id = \"T-D1OfcDW1M\"\n",
    "try:\n",
    "    transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
    "    transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
    "    print(transcript)\n",
    "\n",
    "except TranscriptsDisabled:\n",
    "    print(\"No captions available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89194d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "chunks = splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18147b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb2eb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name\\xa0is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want\\xa0to tell you about a framework to help large language models be more accurate and more up to\\xa0date: Retrieval-Augmented Generation, or RAG. Let\\'s just talk about the \"Generation\" part for a\\xa0minute. So forget the \"Retrieval-Augmented\". So the\\xa0generation, this refers to large')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e194b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yy/_qp4432n43xdq56jtdq_vxsh0000gn/T/ipykernel_25463/1660166801.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "645e66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e8fa62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '1dfbb345-001d-433b-b1c5-ecb63d8569bf',\n",
       " 1: 'b524b8cb-5855-4676-88bd-28cb3483b04b',\n",
       " 2: '37f0831d-c389-4ead-bbc3-73d2695a5d10',\n",
       " 3: '1eb80633-5bd4-4cb0-a0be-c2234c3ab1a4',\n",
       " 4: 'c3a0d5fb-695b-4f0a-b3dc-0ed33f3c9a30',\n",
       " 5: '123c6cf3-bc66-46fd-a959-cfb1e3432ba1',\n",
       " 6: '6d2b03ce-36d9-4baa-bdb4-b3ec8ebd1d03',\n",
       " 7: '26cd9609-cbbe-4622-8a70-6de111a14e15',\n",
       " 8: 'e25b3b82-310f-4815-ae2a-f7f0276bed6f',\n",
       " 9: '6a51bae1-1c9d-4ca8-ba11-af5b31e9df7f',\n",
       " 10: '490c822c-48b5-4c2d-b2aa-a1ae53cfdc29',\n",
       " 11: '5401b0d4-73db-42a9-8315-b3b3a5d2f235',\n",
       " 12: '7660e850-de52-4184-9b55-999ba0f330bb',\n",
       " 13: '34b3067d-cebf-4d97-9044-eb36abb08b87',\n",
       " 14: 'e582efec-2ae1-47cc-99f4-dcf4b1b02b2b'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8330982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1eb80633-5bd4-4cb0-a0be-c2234c3ab1a4', metadata={}, page_content=\"though I confidently said “I read an article, I know the answer!”, I'm not\\xa0sourcing it. I'm giving the answer off the top of my head. And also, I actually haven't kept up with\\xa0this for awhile, and my answer is out of date. So we have two problems here. One is no source.\\xa0And the second problem is that I am out of date.\\xa0\\xa0 And these, in fact, are two behaviors that are\\xa0often observed as problematic when interacting with large language models. They’re LLM\\xa0challenges. Now, what would have happened\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids([\"1eb80633-5bd4-4cb0-a0be-c2234c3ab1a4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1bd3a",
   "metadata": {},
   "source": [
    "Step 2 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fd15ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bc34201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x13fe2cc10>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "369ed9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6a51bae1-1c9d-4ca8-ba11-af5b31e9df7f', metadata={}, page_content='that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll'),\n",
       " Document(id='e582efec-2ae1-47cc-99f4-dcf4b1b02b2b', metadata={}, page_content='to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.'),\n",
       " Document(id='e25b3b82-310f-4815-ae2a-f7f0276bed6f', metadata={}, page_content='Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s'),\n",
       " Document(id='490c822c-48b5-4c2d-b2aa-a1ae53cfdc29', metadata={}, page_content=\"see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I'll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it'll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we're ready. We just go ahead and\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval.invoke(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac9593",
   "metadata": {},
   "source": [
    "Step 3 Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "657abd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant.\n",
    "    Answer ONLY from the provided transcript context.\n",
    "    If the context is insufficient, just say you do not know.\n",
    "\n",
    "    {context}\n",
    "    Question: {question}\n",
    "\n",
    "    \"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9968370e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6a51bae1-1c9d-4ca8-ba11-af5b31e9df7f', metadata={}, page_content='that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll'),\n",
       " Document(id='e25b3b82-310f-4815-ae2a-f7f0276bed6f', metadata={}, page_content='Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s'),\n",
       " Document(id='e582efec-2ae1-47cc-99f4-dcf4b1b02b2b', metadata={}, page_content='to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.'),\n",
       " Document(id='490c822c-48b5-4c2d-b2aa-a1ae53cfdc29', metadata={}, page_content=\"see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I'll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it'll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we're ready. We just go ahead and\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Is the topic of RAG discussed in this video? If yes explain\"\n",
    "retrieved_docs = retrieval.invoke(question)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35dddd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec57db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\n    You are a helpful assistant.\\n    Answer ONLY from the provided transcript context.\\n    If the context is insufficient, just say you do not know.\\n\\n    that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll\\n\\nJupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s\\n\\nto give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.\\n\\nsee, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and\\n    Question: Is the topic of RAG discussed in this video? If yes explain\\n\\n    ')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt = prompt.invoke({\"context\": context_text, \"question\": question})\n",
    "final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe44c0",
   "metadata": {},
   "source": [
    "Step 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d096be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc18688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Yes, the topic of RAG (Retrieval Augmented Generation) is discussed.  The video explains that in the RAG framework, the LLM receives an instruction to retrieve relevant content before answering a user's question.  This retrieved content is combined with the user's question to generate a response.  The speaker also explains how RAG helps address the challenges of LLMs having outdated information and generating poor-quality responses.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--a7c98501-1ae2-415a-9224-2e6fa31dd9b0-0' usage_metadata={'input_tokens': 490, 'output_tokens': 84, 'total_tokens': 574, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(final_prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58830299",
   "metadata": {},
   "source": [
    "Lets make this into a chain now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45677697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "058e37cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_doc(retrieved_docs):\n",
    "    context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e1fcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel({\n",
    "    'context': retrieval | RunnableLambda(format_doc),\n",
    "    'question': RunnablePassthrough()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c987eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll\\n\\nto give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.\\n\\nJupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s\\n\\nsee, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and',\n",
       " 'question': 'What is RAG?'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "113e27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61bfada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_chain = parallel_chain | prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47d8074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The video describes Retrieval Augmented Generation (RAG).  In RAG, a large language model (LLM) receives a user's question. Instead of immediately generating an answer, it first retrieves relevant content from a content store (which could be the internet or a closed collection of documents).  The LLM then combines this retrieved content with the user's question to generate a final, more accurate and evidence-based answer.  This addresses two challenges of LLMs: providing high-quality data for grounding the response and enabling richer, better responses.  The example given shows how a question about which planet has rings would be answered differently: a standard LLM might incorrectly say Jupiter, while a RAG-based LLM would retrieve relevant information and correctly identify Saturn.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_chain.invoke(\"can you summarize the video?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
